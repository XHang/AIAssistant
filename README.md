# **本地多语言 AI 模型需求文档 **

## **1. 项目目标**

构建一个可在本地运行的 AI 模型系统，满足以下核心需求：

1. 支持 **中文 / 日文 / 英文** 三语互相翻译  
2. 支持 **简单对话能力**  
3. **无审查、无主题限制**  
4. **最低配置电脑也能运行**  
5. 使用 **一个基础模型（Qwen3‑3B）**  
6. 提供 **多个量化版本**，适配不同性能机器  
7. **跨平台运行**（Windows / Linux / Android）  
8. **GPU 可选**：如果电脑有 GPU，可通过配置切换到 GPU 推理  
9. 模型选择属于 **初始配置**，一旦确定不可随意更换  
   - 更换模型 = 训练成果全部失效  

---

## **2. 最低运行环境（硬性要求）**

系统必须在以下最低配置上可运行：

- **CPU：Intel i5‑7400（3GHz）**  
- **内存：16GB**  
- **GPU：GTS450（或无 GPU）**  
- **操作系统：Windows / Linux / Android（Termux 或 MLC）**

在此环境下：

- 使用 **最低量化（q2_K / q0f16）** 的模型  
- 翻译速度可慢，但必须能完成  
- 对话能力可弱，但必须能响应  

---

## **3. 模型要求**

### **3.1 基础模型**

- 固定使用 **Qwen3‑3B** 作为项目唯一基础模型  
- 所有训练成果与该模型绑定  

### **3.2 量化版本**

必须为同一个模型提供多个量化版本：

- **q2_K**（最低配置模式）  
- **q4_K / q4_K_M**（普通电脑）  
- **q5_K**（中高性能电脑）  
- **fp16**（高性能电脑）

所有量化版本共享同一训练成果。

### **3.3 模型切换规则**

- 模型选择属于 **项目初始化阶段**  
- 一旦用户开始训练，模型不可更换  
- 更换模型 = 训练成果全部失效  
- 量化版本可自由切换（不影响训练成果）

---

## **4. 功能需求**

### **4.1 翻译功能**

- 中文 ↔ 日文  
- 中文 ↔ 英文  
- 日文 ↔ 英文  
- 支持短句与中等长度文本  

### **4.2 对话功能**

- 支持简单问答  
- 支持三语混合  
- 不进行内容审查  

### **4.3 自动性能适配**

系统启动时自动检测硬件：

- CPU 核心数  
- 内存大小  
- GPU 型号  
- GPU 显存  

根据检测结果自动选择：

- 量化版本  
- 线程数  
- 上下文长度（ctx）  
- 是否启用 GPU  

但 **不自动切换模型本体**。

---

## **5. GPU 切换要求**

系统必须支持：

- **CPU 模式**  
- **GPU 模式（CUDA / Vulkan / Metal）**

切换方式：

- 通过 **配置文件** 切换  
- 不需要重新训练  
- 不需要重新量化  
- 不需要更换模型文件  

示例配置：

```
use_gpu = true
gpu_backend = "cuda"
```

换电脑后，只需修改配置即可切换 GPU。

---

## **6. 训练与持久化**

### **6.1 训练方式**

- 用户可对当前模型进行微调或增量训练  
- 训练成果与当前模型绑定  

### **6.2 训练成果兼容性**

- 同一模型的不同量化版本 **共享训练成果**  
- 不同模型之间 **不共享训练成果**

---

## **7. 跨平台要求**

最终成品必须支持：

- **Windows（x86_64）**  
- **Linux（x86_64）**  
- **Android（Termux 或 MLC 原生）**

要求：

- 模型文件跨平台通用  
- 推理核心可替换（llama.cpp / MLC）  
- 启动脚本根据平台自动选择对应可执行文件  

---

## **8. 系统架构改进**

### **8.1 异步消息队列架构**

最新的系统采用了异步消息队列架构，主要改进包括：

1. **UI与AI处理分离**：GUI界面运行在主线程，AI对话处理在后台线程
2. **消息队列通信**：使用生产者-消费者模式进行线程间通信
3. **实时流式输出**：支持AI回复的实时流式显示
4. **非阻塞操作**：用户界面不会因AI处理而卡顿

### **8.2 核心组件**

- `message_queue.py`：消息队列管理器，处理GUI与工作线程间的通信
- `conversation_worker.py`：AI对话工作线程，负责与LlamaServer通信
- `main.py`：改进的GUI界面，支持异步对话

### **8.3 工作流程**

1. 用户在GUI中输入对话内容
2. 点击"对话"按钮将消息放入输入队列
3. 后台工作线程从队列获取消息并发送给LlamaServer
4. 工作线程接收AI的流式回复并放入输出队列
5. GUI定时器定期检查输出队列并更新显示
6. 实现流畅的异步对话体验

---

## **9. 使用说明**

### **9.1 启动系统**

```bash
# 启动主程序
python main.py

# 或运行测试脚本
python test_conversation.py
```

### **9.2 配置文件**

`config.json` 配置示例：
```json
{
  "model": "D:/ai/model/Qwen3-4B-Instruct-2507-Q4_K_M.gguf",
  "ctx_size": 4096,
  "threads": 8,
  "port": 8080
}
```

### **9.3 功能特点**

- **异步对话**：真正的异步处理，界面响应流畅
- **流式输出**：AI回复实时显示，用户体验更好
- **对话历史**：维护对话上下文，支持连续对话
- **错误处理**：完善的异常处理机制
- **资源管理**：合理的线程管理和资源释放

---