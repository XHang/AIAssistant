# **本地多语言 AI 模型需求文档 **

## **1. 项目目标**

构建一个可在本地运行的 AI 模型系统，满足以下核心需求：

1. 支持 **中文 / 日文 / 英文** 三语互相翻译  
2. 支持 **简单对话能力**  
3. **无审查、无主题限制**  
4. **最低配置电脑也能运行**  
5. 使用 **一个基础模型（Qwen3‑3B）**  
6. 提供 **多个量化版本**，适配不同性能机器  
7. **跨平台运行**（Windows / Linux / Android）  
8. **GPU 可选**：如果电脑有 GPU，可通过配置切换到 GPU 推理  
9. 模型选择属于 **初始配置**，一旦确定不可随意更换  
   - 更换模型 = 训练成果全部失效  

---

## **2. 最低运行环境（硬性要求）**

系统必须在以下最低配置上可运行：

- **CPU：Intel i5‑7400（3GHz）**  
- **内存：16GB**  
- **GPU：GTS450（或无 GPU）**  
- **操作系统：Windows / Linux / Android（Termux 或 MLC）**

在此环境下：

- 使用 **最低量化（q2_K / q0f16）** 的模型  
- 翻译速度可慢，但必须能完成  
- 对话能力可弱，但必须能响应  

---

## **3. 模型要求**

### **3.1 基础模型**

- 固定使用 **Qwen3‑3B** 作为项目唯一基础模型  
- 所有训练成果与该模型绑定  

### **3.2 量化版本**

必须为同一个模型提供多个量化版本：

- **q2_K**（最低配置模式）  
- **q4_K / q4_K_M**（普通电脑）  
- **q5_K**（中高性能电脑）  
- **fp16**（高性能电脑）

所有量化版本共享同一训练成果。

### **3.3 模型切换规则**

- 模型选择属于 **项目初始化阶段**  
- 一旦用户开始训练，模型不可更换  
- 更换模型 = 训练成果全部失效  
- 量化版本可自由切换（不影响训练成果）

---

## **4. 功能需求**

### **4.1 翻译功能**

- 中文 ↔ 日文  
- 中文 ↔ 英文  
- 日文 ↔ 英文  
- 支持短句与中等长度文本  

### **4.2 对话功能**

- 支持简单问答  
- 支持三语混合  
- 不进行内容审查  

### **4.3 自动性能适配**

系统启动时自动检测硬件：

- CPU 核心数  
- 内存大小  
- GPU 型号  
- GPU 显存  

根据检测结果自动选择：

- 量化版本  
- 线程数  
- 上下文长度（ctx）  
- 是否启用 GPU  

但 **不自动切换模型本体**。

---

## **5. GPU 切换要求**

系统必须支持：

- **CPU 模式**  
- **GPU 模式（CUDA / Vulkan / Metal）**

切换方式：

- 通过 **配置文件** 切换  
- 不需要重新训练  
- 不需要重新量化  
- 不需要更换模型文件  

示例配置：

```
use_gpu = true
gpu_backend = "cuda"
```

换电脑后，只需修改配置即可切换 GPU。

---

## **6. 训练与持久化**

### **6.1 训练方式**

- 用户可对当前模型进行微调或增量训练  
- 训练成果与当前模型绑定  

### **6.2 训练成果兼容性**

- 同一模型的不同量化版本 **共享训练成果**  
- 不同模型之间 **不共享训练成果**

---

## **7. 跨平台要求**

最终成品必须支持：

- **Windows（x86_64）**  
- **Linux（x86_64）**  
- **Android（Termux 或 MLC 原生）**

要求：

- 模型文件跨平台通用  
- 推理核心可替换（llama.cpp / MLC）  
- 启动脚本根据平台自动选择对应可执行文件  

---

## **8. 非功能需求**

- 完全本地运行  
- 不依赖网络  
- 不上传数据  
- 不进行内容过滤  
- 稳定运行，不崩溃  
- 目录结构清晰  

---

# 选定的模型

[Qwen3‑4B‑Instruct‑2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)

# 选择的模型框架

[llama.cpp](https://github.com/ggml-org/llama.cpp)

## 安装过程(linux)

### 安装nix

先安装[nix](https://nixos.org/download/)

注意：nix默认关闭了一些功能，启动

```
mkdir -p ~/.config/nix
printf "experimental-features = nix-command flakes\n" >> ~/.config/nix/nix.conf

```

```
source ~/.profile 2>/dev/null || true
source ~/.bashrc 2>/dev/null || true

```

### 下载模型

一般huggingface模型页面右上方有三个小点的按钮，点开，选择Clone repository，可以使用git模式，但是git clone 默认不下载大文件，所以先让它支持大文件
`sudo apt install git-lfs`

然后再下载，文件很大，需要等待。



## 安装过程(win)

没啥好说的，如果有winGet，直接按说明文档来安装，没有任何问题，注意git版本可能要换个支持大文件的版本，huggingface也很人性化的帮你写好了命令去安装，当然还是winGet，然后直接下载模型，没啥技术含量

## 转换模型

我写了一个bat脚本,按照脚本的提示一步步操作就行了，linux也写了个，不过没测试。算了




